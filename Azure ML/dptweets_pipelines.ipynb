{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mental Health Ops - Detect Depressive Sentiment from Tweets\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Azure Machine Learning Imports\n",
        "\n",
        "In this first code cell, we import key Azure Machine Learning modules that we will use below. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Experiment, Datastore\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.51.0\n"
        }
      ],
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1702471719358
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline-specific SDK imports\n",
        "\n",
        "Here, we import key pipeline modules"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "print(\"Pipeline SDK-specific imports completed\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline SDK-specific imports completed\n"
        }
      ],
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1702471728640
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Workspace\n",
        "\n",
        "Initializing a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
        "\n",
        "# Default datastore\n",
        "def_blob_store = ws.get_default_datastore() \n",
        "# The following call GETS the Azure Blob Store associated with the workspace. \n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "dp-tweets\nmy_mlops_prj\neastus2\n34a65394-13fc-43ed-8e03-a0f81df6e347\nBlobstore's name: workspaceblobstore\n"
        }
      ],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702471733451
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload data to Blob Store\n",
        "This code snippet is used to focus on uploading files (tweets and a machine learning model) to an Azure Blob Storage associated"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "from azureml.data.data_reference import DataReference\n",
        "\n",
        "\n",
        "tweet1 = \"./data/depressive_tweets.csv\"\n",
        "tweet2 = \"./data/random_tweets.csv\"\n",
        "modelsvm = \"./models/model_svm1.pkl\"\n",
        "\n",
        "#Uploading Depressive Tweets\n",
        "with open(tweet1, \"r\") as f:\n",
        "   # get_default_datastore() gets the default Azure Blob Store associated with the workspace.\n",
        "   # Here we are reusing the def_blob_store object we obtained earlier\n",
        "    def_blob_store.upload_files([tweet1], overwrite=True)\n",
        "print(\"Depressive Tweets: Upload call completed\")\n",
        "\n",
        "#Uploading Random Tweets\n",
        "with open(tweet2, \"r\") as f:\n",
        "   # get_default_datastore() gets the default Azure Blob Store associated with the workspace.\n",
        "   # Here we are reusing the def_blob_store object we obtained earlier\n",
        "    def_blob_store.upload_files([tweet2], overwrite=True)\n",
        "print(\"Random Tweets: Upload call completed\")\n",
        "\n",
        "#Uploading Machine Learning Model  \n",
        "\n",
        "with open(modelsvm, \"r\") as f:\n",
        "   # get_default_datastore() gets the default Azure Blob Store associated with the workspace.\n",
        "   # Here we are reusing the def_blob_store object we obtained earlier\n",
        "    def_blob_store.upload_files([modelsvm], target_path=\"models\", overwrite=True)\n",
        "print(\"Random Tweets: Upload call completed\")\n",
        "\n",
        "'''\n",
        "my_dataset1 = Dataset.File.from_files([(def_blob_store, 'tw-data/random_tweets.csv')])\n",
        "my_dataset2 = Dataset.File.from_files([(def_blob_store, 'tw-data/depressive_tweets.csv')])\n",
        "'''\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 1 files\nUploading ./data/depressive_tweets.csv\nUploaded ./data/depressive_tweets.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nDepressive Tweets: Upload call completed\nUploading an estimated of 1 files\nUploading ./data/random_tweets.csv\nUploaded ./data/random_tweets.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nRandom Tweets: Upload call completed\nUploading an estimated of 1 files\nUploading ./models/model_svm1.pkl\nUploaded ./models/model_svm1.pkl, 1 files out of an estimated total of 1\nUploaded 1 files\nRandom Tweets: Upload call completed\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "\"\\nmy_dataset1 = Dataset.File.from_files([(def_blob_store, 'tw-data/random_tweets.csv')])\\nmy_dataset2 = Dataset.File.from_files([(def_blob_store, 'tw-data/depressive_tweets.csv')])\\n\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702471755354
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### List of Compute Targets on the workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cts = ws.compute_targets\n",
        "for ct in cts:\n",
        "    print(ct)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "x221040381\ncpu-cluster\ncpu-cluster-ml\n"
        }
      ],
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1702471762169
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieve or create a Azure Machine Learning compute\n",
        "1. Creating the configuration\n",
        "2. Creating the Azure Machine Learning compute\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Attempts to retrieve an existing compute target with the specified name \n",
        "aml_compute_target = \"cpu-cluster\"\n",
        "try:\n",
        "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
        "    print(\"found existing compute target.\")\n",
        "except ComputeTargetException:\n",
        "    print(\"creating new compute target\")\n",
        "#If the compute target doesn't exist, the code proceeds to create a new compute target named     \n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
        "                                                                min_nodes = 1, \n",
        "                                                                max_nodes = 4)    \n",
        "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
        "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "print(\"Azure Machine Learning Compute attached\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "found existing compute target.\nAzure Machine Learning Compute attached\n"
        }
      ],
      "execution_count": 55,
      "metadata": {
        "gather": {
          "logged": 1702471767089
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a more detailed view of current Azure Machine Learning Compute status\n",
        "\n",
        "print(aml_compute.get_status().serialize())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2023-12-09T17:28:08.456000+00:00', 'errors': None, 'creationTime': '2023-12-09T16:23:25.454351+00:00', 'modifiedTime': '2023-12-09T16:23:35.280897+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1702471789909
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Pipeline\n",
        "This is to setup a run configuration for Azure Machine Learning that includes specifications for the Docker environment and Conda dependencies."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "\n",
        "# create a new runconfig object\n",
        "run_config = RunConfiguration()\n",
        "\n",
        "# enable Docker \n",
        "run_config.environment.docker.enabled = True\n",
        "\n",
        "# set Docker base image to the default CPU-based image\n",
        "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "\n",
        "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
        "run_config.environment.python.user_managed_dependencies = False\n",
        "\n",
        "# specify CondaDependencies obj\n",
        "#run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
        "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=[\"pip\", \"python=3.8\", \"pandas==1.3.4\", \"numpy\", \"matplotlib\"], \n",
        "pip_packages=['blis==0.4.1',\n",
        "'certifi==2021.10.8',\n",
        "'charset-normalizer==2.0.7',\n",
        "'click==8.0.3',\n",
        "'cycler==0.11.0',\n",
        "'cymem==2.0.6',\n",
        "'fonttools==4.28.1',\n",
        "'ftfy==6.0.3',\n",
        "'idna==3.3',\n",
        "'joblib==1.1.0',\n",
        "'kiwisolver==1.3.2',\n",
        "'murmurhash==1.0.6',\n",
        "'nltk==3.6.5',\n",
        "'packaging==21.2',\n",
        "'Pillow==8.4.0',\n",
        "'plac==0.9.6',\n",
        "'preshed==3.0.6',\n",
        "'pyparsing==2.4.7',\n",
        "'python-dateutil==2.8.2',\n",
        "'pytz==2021.3',\n",
        "'regex==2021.11.10',\n",
        "'requests==2.26.0',\n",
        "'scikit-learn==1.0.1',\n",
        "'scipy==1.7.2',\n",
        "'seaborn==0.11.2',\n",
        "'setuptools==57.0.0',\n",
        "'setuptools-scm==6.3.2',\n",
        "'six==1.16.0',\n",
        "'spacy==2.2.3',\n",
        "'srsly==1.0.5',\n",
        "'thinc==7.3.0',\n",
        "'threadpoolctl==3.0.0',\n",
        "'tomli==1.2.2',\n",
        "'tqdm==4.62.3',\n",
        "'typing-extensions==4.7.1',\n",
        "'urllib3==1.26.7',\n",
        "'wasabi==0.8.2',\n",
        "'wcwidth==0.2.5'\n",
        "])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702471801269
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data refernece\n",
        "\n",
        "This is part of a pipeline and is setting up a PythonScriptStep for a data preprocessing task"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineData\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.data.data_reference import DataReference\n",
        "\n",
        "# Uses default values for PythonScriptStep construct.\n",
        "\n",
        "source_directory = './src'\n",
        "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
        "# Referencing Depression tweets\n",
        "dep_data = DataReference(\n",
        "        datastore=def_blob_store,\n",
        "        data_reference_name=\"dep_data\",\n",
        "        path_on_datastore=os.path.join(\"dep_data\",\"depressive_tweets.csv\"),\n",
        "    )\n",
        "print(\"DataReference object1 created\")\n",
        "#Referencing Random tweets\n",
        "rand_data = DataReference(\n",
        "        datastore=def_blob_store,\n",
        "        data_reference_name=\"rand_data\",\n",
        "        path_on_datastore=os.path.join(\"rand_data\",\"random_tweets.csv\"),\n",
        "    )\n",
        "print(\"DataReference object2 created\")\n",
        "\n",
        "preprocessed_data = PipelineData('preprocessed_data')\n",
        "#This is used to represent the output data of the pipeline step\n",
        "\n",
        "step1 = PythonScriptStep(name=\"preprocess_step\",\n",
        "                         script_name=\"preprocess.py\", \n",
        "                         arguments=[\"--dep_data\", dep_data, \"--rand_data\", rand_data, \"--preprocessed_data\",\n",
        "                         preprocessed_data],\n",
        "                         inputs = [dep_data, rand_data],\n",
        "                         outputs= [preprocessed_data],\n",
        "                         compute_target=aml_compute, \n",
        "                         source_directory=source_directory,\n",
        "                         runconfig=run_config,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step1 created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Source directory for the step is /mnt/batch/tasks/shared/LS_root/mounts/clusters/cpu-cluster-ml/code/Users/x22104038/src.\nDataReference object1 created\nDataReference object2 created\nStep1 created\n"
        }
      ],
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1702471815427
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this step, we use a different source_directory\n",
        "source_directory = './src'\n",
        "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
        "# Creating DataReference for Preprocessed Data\n",
        "preprocessed_data = DataReference(\n",
        "        datastore=def_blob_store,\n",
        "        data_reference_name=\"preprocessed_data\",\n",
        "        path_on_datastore=os.path.join(\"preprocessed_data.csv\"),\n",
        "    )\n",
        "print(\"DataReference object4 created\")\n",
        "\n",
        "#This step is intended for training a machine learning model using the preprocessed data \n",
        "# All steps use the same Azure Machine Learning compute target as well\n",
        "step2 = PythonScriptStep(name=\"training_step\",\n",
        "                         script_name=\"modeltrain.py\", \n",
        "                         arguments=[\"--preprocessed_data\", preprocessed_data],\n",
        "                         inputs = [preprocessed_data],\n",
        "                         compute_target=aml_compute, \n",
        "                         source_directory=source_directory,\n",
        "                         runconfig=run_config)\n",
        "step2.run_after(step1)\n",
        "#The second step depends on the output of the first step, ensuring a sequential execution order in the pipeline\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Source directory for the step is /mnt/batch/tasks/shared/LS_root/mounts/clusters/cpu-cluster-ml/code/Users/x22104038/src.\nDataReference object4 created\n"
        }
      ],
      "execution_count": 59,
      "metadata": {
        "gather": {
          "logged": 1702471843254
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediciton"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this step, we use yet another source_directory\n",
        "# Predictions using a trained machine learning model\n",
        "source_directory = './src'\n",
        "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\n",
        "\n",
        "step3 = PythonScriptStep(name=\"prediction\",\n",
        "                         script_name=\"prediction.py\", \n",
        "                         compute_target=aml_compute, \n",
        "                         source_directory=source_directory,\n",
        "                         runconfig=run_config)\n",
        "step3.run_after(step2)\n",
        "#This step is designed for making predictions using a trained machine learning model\n",
        "# Prediction after Training\n",
        "# list of steps to run\n",
        "steps = [step1, step2, step3]\n",
        "\n",
        "print(\"Step lists created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Source directory for the step is /mnt/batch/tasks/shared/LS_root/mounts/clusters/cpu-cluster-ml/code/Users/x22104038/src.\nStep lists created\n"
        }
      ],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702471883786
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the pipeline\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax\n",
        "# Pipeline(workspace, \n",
        "#          steps, \n",
        "#          description=None, \n",
        "#          default_datastore_name=None, \n",
        "#          default_source_directory=None, \n",
        "#          resolve_closure=True, \n",
        "#          _workflow_provider=None, \n",
        "#          _service_endpoint=None)\n",
        "\n",
        "pipeline1 = Pipeline(workspace=ws, steps=steps)\n",
        "print (\"Pipeline is built\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built\n"
        }
      ],
      "execution_count": 61,
      "metadata": {
        "gather": {
          "logged": 1702471892700
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate the pipeline\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline1.validate()\n",
        "print(\"Pipeline validation complete\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline validation complete\n"
        }
      ],
      "execution_count": 62,
      "metadata": {
        "gather": {
          "logged": 1702471895547
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit syntax\n",
        "# submit(experiment_name, \n",
        "#        pipeline_parameters=None, \n",
        "#        continue_on_step_failure=False, \n",
        "#        regenerate_outputs=False)\n",
        "\n",
        "pipeline_run1 = Experiment(ws, 'Depression_Tweets').submit(pipeline1, regenerate_outputs=False)\n",
        "print(\"Pipeline is submitted for execution\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step preprocess_step [e043d517][9eb0fc25-c483-4c48-b466-784f00805a13], (This step is eligible to reuse a previous run's output)\nCreated step training_step [64bdee9d][c71a5ad9-84ca-4eef-a9f9-542fb94fb05e], (This step is eligible to reuse a previous run's output)\nCreated step prediction [3ecbcc88][9449445c-cfee-44c7-85e8-009bc5b59883], (This step is eligible to reuse a previous run's output)\nUsing data reference dep_data for StepId [a57e9120][5ec54ca7-e6ac-406f-802e-74fe8ac91e59], (Consumers of this data are eligible to reuse prior runs.)\nUsing data reference rand_data for StepId [5bdcd7d8][afe62eae-3fa3-4e15-9519-57ba4b7bcc08], (Consumers of this data are eligible to reuse prior runs.)\nUsing data reference preprocessed_data for StepId [9dba6346][20022763-cfd2-4e69-b7cc-1a12d4492b2b], (Consumers of this data are eligible to reuse prior runs.)\nSubmitted PipelineRun 71e05035-08d3-4e9f-904e-366e69c47c3d\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/71e05035-08d3-4e9f-904e-366e69c47c3d?wsid=/subscriptions/34a65394-13fc-43ed-8e03-a0f81df6e347/resourcegroups/my_mlops_prj/workspaces/dp-tweets&tid=6edb49c1-bf72-4eea-8b3f-a7fd0a25b68c\nPipeline is submitted for execution\n"
        }
      ],
      "execution_count": 63,
      "metadata": {
        "gather": {
          "logged": 1702471899451
        }
      }
    }
  ],
  "metadata": {
    "order_index": 1,
    "exclude_from_index": false,
    "task": "Getting Started notebook for ANML Pipelines",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "Custom"
    ],
    "category": "tutorial",
    "framework": [
      "Azure ML"
    ],
    "friendly_name": "Getting Started with Azure Machine Learning Pipelines",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}